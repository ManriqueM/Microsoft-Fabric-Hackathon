{"cells":[{"cell_type":"markdown","id":"f5f8b248-2972-4df4-ad7f-3e2d4128d9ba","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Install packages"]},{"cell_type":"code","execution_count":null,"id":"654c69a2-bdc8-4dbd-855a-7be96041d672","metadata":{},"outputs":[],"source":["%pip install openai==1.12.0"]},{"cell_type":"markdown","id":"b1f6884c-2f65-4e60-98ca-2afd61b53ba1","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Required packages"]},{"cell_type":"code","execution_count":null,"id":"23ae6e46-de9c-4069-b2af-7904b67142b6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import openai\n","from openai import AzureOpenAI\n","import json\n","from IPython import get_ipython\n","from IPython.terminal.interactiveshell import TerminalInteractiveShell\n","import uuid\n","import mlflow\n","\n","from pyspark.sql import functions as F\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.sql.types import *\n","from pyspark.ml import Pipeline\n","\n","from synapse.ml.isolationforest import *\n","\n","from synapse.ml.explainers import *\n","\n","%matplotlib inline\n","\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import IntegerType, DoubleType"]},{"cell_type":"markdown","id":"3899873d-527a-4433-8de4-0e1fe17c6a21","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Set up Azure OpenAI connection\n","###### Reference: https://community.fabric.microsoft.com/t5/Hack-Together/Fabric-cant-import-AzureOpenAI/m-p/3703267#M66"]},{"cell_type":"code","execution_count":null,"id":"e25c2a4b-6db7-4736-b0e7-a0030cca9652","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["client = AzureOpenAI(\n","    # Include endpoint - for the competition it was https://polite-ground-030dc3103.4.azurestaticapps.net/api/v1\n","\n","\n","\n","\n","\n","    azure_endpoint=\"ADD ENDPOINT\",\n","    api_key=\"YOUR API KEY\",  # Add API KEY\n","    api_version=\"2023-09-01-preview\",\n","\n","\n","\n","\n","\n",")"]},{"cell_type":"code","execution_count":null,"id":"89f826f0-d027-4763-9288-0f8255419295","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Assuming df is your DataFrame loaded from the delta format\n","df = spark.read.format(\"delta\").load(\n","    \"abfss://MS_Fabric_Hackathon@onelake.dfs.fabric.microsoft.com/Hackathon.Lakehouse/Tables/Fact_Sales\")\n","df.dtypes"]},{"cell_type":"markdown","id":"123ddb54","metadata":{},"source":["### Get Data"]},{"cell_type":"code","execution_count":null,"id":"b660fd4b","metadata":{},"outputs":[],"source":["df = spark.read.format(\"delta\").load(\n","    \"abfss://MS_Fabric_Hackathon@onelake.dfs.fabric.microsoft.com/Hackathon.Lakehouse/Tables/Fact_Sales\")\n","\n","\n","\n","# Ensure 'Quantity' and 'Unit_Price' are treated as numeric types\n","\n","\n","df = df.withColumn(\"Quantity\", F.col(\"Quantity\").cast(IntegerType())) \\\n","       .withColumn(\"Unit_Price\", F.col(\"Unit_Price\").cast(FloatType()))\n","\n","\n","# Perform aggregation\n","\n","\n","df = df.groupBy(\"Date\") \\\n","    .agg(\n","        F.sum(\"Quantity\").alias(\"Quantity\"),  # Sum of Quantity\n","        (F.sum(F.col(\"Quantity\") * F.col(\"Unit_Price\")) / F.sum(\"Quantity\")\n","         ).alias(\"Unit_Price\")  # Average Price calculation\n",")\n","\n","\n","\n","# Cast columns as needed\n","\n","\n","\n","df_adjusted = (\n","    df.orderBy(\"Date\")\n","    .withColumn(\"Date\", col(\"Date\").cast(DateType()))\n","    .withColumn(\"Quantity\", col(\"Quantity\").cast(IntegerType()))\n","    .withColumn(\"Unit_Price\", col(\"Unit_Price\").cast(FloatType()))\n","\n","\n","\n","\n","    .select(\n","        \"Date\",\n","        \"Quantity\",\n","        \"Unit_Price\"\n","    )\n",")\n","\n","\n","\n","# Convert Spark DataFrame to Pandas DataFrame\n","\n","\n","pd_df_adjusted = df_adjusted.toPandas()\n","\n","\n","pd_df_adjusted"]},{"cell_type":"markdown","id":"995a39e2-cb12-406d-845a-a56e55a5aacb","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Create prompt"]},{"cell_type":"code","execution_count":null,"id":"08542df2-f19b-42ac-98e0-8a99ff932bcc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Prompt is a combination of 3 parts:\n","\n","prompt_1 = \"Given this data: \"\n","prompt_2 = pd_df_adjusted\n","prompt_3 = \"\"\"\n","Please run an Isolation Forest model with the parameters below and share the results.\n","\n","contamination=0.1, \n","n_estimators=16,\n","max_samples=210, \n","max_features=1.0\n","trainingStartTime = \"2023-01-01\"\n","trainingEndTime = \"2023-07-31\"\n","inferenceStartTime = \"2023-08-01\"\n","inferenceEndTime = \"2023-08-31\"\n","\n","\"\"\"\n","\n","# Combine the 3 into a single text\n","PROMPT = f\"{prompt_1}\\n\\n{prompt_2}\\n\\n{prompt_3}\"\n","\n","print(PROMPT)"]},{"cell_type":"code","execution_count":null,"id":"4f4729f6-455a-481f-bbbb-306b31c9d853","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Prompt is a combination of 3 parts:\n","\n","prompt_1 = \"Given this data: \"\n","prompt_2 = pd_df_adjusted\n","prompt_3 = \"\"\"\n","Can you provide the code to show the Isolation Forest model results with these parameters. \n","Only provide the python code in your response, please don't include any words different from the code.Please exclude any comments in the response, just provide the code\n","\n","Please consider these parameters for the model:\n","contamination=0.1, \n","n_estimators=100, 16\n","max_samples=210, \n","max_features=1.0\n","trainingStartTime = \"2023-01-01\"\n","trainingEndTime = \"2023-07-31\"\n","inferenceStartTime = \"2023-08-01\"\n","inferenceEndTime = \"2023-08-31\"\n","\n","Please ensure all arrays provided are of the same length\n","Please use the data from the dataframe pd_df_adjusted provided in the prompt, do not try to load or add other data.\n","To run the model, split the data for training and test, according to the start and end times provided\n","\n","\"\"\"\n","\n","# Combine the 3 into a single text\n","PROMPT = f\"{prompt_1}\\n\\n{prompt_2}\\n\\n{prompt_3}\"\n","\n","print(PROMPT)"]},{"cell_type":"markdown","id":"94256722-a07e-4786-9f0d-738e95c098eb","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Get AzureOpenAI response"]},{"cell_type":"code","execution_count":null,"id":"a3a7d109-44cd-4b8d-93e5-f3eb7026658b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Run using AzureOpenAI - Select the model and temperature\n","MESSAGES = []\n","MESSAGES.append({\"role\": \"user\", \"content\": PROMPT})\n","completion = client.chat.completions.create(\n","    # model and temperature adjusted as suggested here: https://www.reddit.com/r/ChatGPTCoding/comments/12i6k06/best_temperature_for_gpt4_api_to_get_quality/\n","    model=\"gpt-4\", messages=MESSAGES, temperature=0.1)\n","response = completion.choices[0].message.content\n","print(response)"]},{"cell_type":"markdown","id":"f61ba2c8-846e-425a-8e60-d405b4c8f6ca","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["### Execute response"]},{"cell_type":"code","execution_count":null,"id":"58b59924-86af-4bcc-a86a-ab825f0aca60","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["cleaned_response = response.replace(\"python\\n\", \"\").replace(\"\", \"\")\n","cleaned_response = cleaned_response.replace(\"```\", \"\")\n","print(cleaned_response)\n","exec(cleaned_response)"]},{"cell_type":"markdown","id":"73944b3b","metadata":{},"source":["### Display results"]},{"cell_type":"code","execution_count":null,"id":"64ddb7ac-61a8-4f35-b86e-92dfc79a26cc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import numpy as np\n","\n","# Add predictions back to the inference DataFrame\n","# Note: The predictions are -1 for outliers and 1 for inliers\n","inference_data['Anomaly'] = anomalies\n","\n","# If desired, convert anomalies from -1/1 to a more readable format (e.g., True/False or \"Outlier\"/\"Inlier\")\n","inference_data['Anomaly'] = np.where(\n","    inference_data['Anomaly'] == -1, 'Outlier', 'Inlier')\n","\n","# Display the updated inference data with anomaly predictions\n","print(inference_data)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"trident":{"lakehouse":{"default_lakehouse":"ee911489-4713-4e55-aedb-ebc5e3235b6b","default_lakehouse_name":"Hackathon","default_lakehouse_workspace_id":"4c4664f3-622d-4959-933a-efeec33fe813"}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
